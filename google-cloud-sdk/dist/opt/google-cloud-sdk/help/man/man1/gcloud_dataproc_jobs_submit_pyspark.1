
.TH "GCLOUD_DATAPROC_JOBS_SUBMIT_PYSPARK" 1



.SH "NAME"
.HP
gcloud dataproc jobs submit pyspark \- submit a PySpark job to a cluster



.SH "SYNOPSIS"
.HP
\f5gcloud dataproc jobs submit pyspark\fR \fIPY_FILE\fR \fB\-\-cluster\fR \fICLUSTER\fR [\fB\-\-archives\fR\ [\fIARCHIVE\fR,...]] [\fB\-\-async\fR] [\fB\-\-bucket\fR\ \fIBUCKET\fR] [\fB\-\-driver\-log\-levels\fR\ [\fIPACKAGE\fR=\fILEVEL\fR,...]] [\fB\-\-files\fR\ [\fIFILE\fR,...]] [\fB\-\-properties\fR\ [\fIPROPERTY\fR=\fIVALUE\fR,...]] [\fB\-\-py\-files\fR\ [\fIPY_FILE\fR,...]] [\fIGLOBAL\-FLAG\ ...\fR] [\fIJOB_ARGS\fR\ ...]



.SH "DESCRIPTION"

Submit a PySpark job to a cluster.



.SH "POSITIONAL ARGUMENTS"

\fIPY_FILE\fR
.RS 2m
The main .py file to run as the driver.

.RE
[\fIJOB_ARGS\fR ...]
.RS 2m
The arguments to pass to the driver.


.RE

.SH "REQUIRED FLAGS"

\fB\-\-cluster\fR \fICLUSTER\fR
.RS 2m
The Dataproc cluster to submit the job to.


.RE

.SH "OPTIONAL FLAGS"

\fB\-\-archives\fR [\fIARCHIVE\fR,...]
.RS 2m
Comma separated list of archives to be provided to the job. must be one of the
following file formats: .zip, .tar, .tar.gz, or .tgz.

.RE
\fB\-\-async\fR
.RS 2m
Does not wait for the job to run.

.RE
\fB\-\-bucket\fR \fIBUCKET\fR
.RS 2m
The Cloud Storage bucket to stage files in. Defaults to the cluster's configured
bucket.

.RE
\fB\-\-driver\-log\-levels\fR [\fIPACKAGE\fR=\fILEVEL\fR,...]
.RS 2m
A list of package to log4j log level pairs to configure driver logging. For
example: root=FATAL,com.example=INFO

.RE
\fB\-\-files\fR [\fIFILE\fR,...]
.RS 2m
Comma separated list of files to be provided to the job.

.RE
\fB\-\-properties\fR [\fIPROPERTY\fR=\fIVALUE\fR,...]
.RS 2m
A list of key value pairs to configure PySpark.

.RE
\fB\-\-py\-files\fR [\fIPY_FILE\fR,...]
.RS 2m
Comma separated list of Python files to be provided to the job.Must be one of
the following file formats" .py, ,.zip, or .egg


.RE

.SH "GLOBAL FLAGS"

Run \fB$ gcloud help\fR for a description of flags available to all commands.



.SH "EXAMPLES"

To submit a PySpark job with a local script, run:

.RS 2m
$ gcloud dataproc jobs submit pyspark \-\-cluster my_cluster \e
    my_script.py
.RE

To submit a Spark job that runs a script that is already on the cluster, run:

.RS 2m
$ gcloud dataproc jobs submit pyspark \-\-cluster my_cluster \e
    file:///usr/lib/spark/examples/src/main/python/pi.py 100
.RE
